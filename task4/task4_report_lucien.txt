We used LinUCB with disjoint linear models. In every round our recommend function receives an action set and a feature vector (in the task this corresponds to a set of articles from which we choose a recommendation and the user features). For each action then a UCB-score is computed, which basically just linearly weighs the user feature vector and adds some confidence interval cushion. The true weight vector associated with each action then is contained in a confidence ellipsoid. The recommended action is the one, which maximizes its upper confidence bound on the feature vector.
Only if the chosen action matches the log-file, the policy is evaluated and the update method is called. We receive some reward which indicates whether the user clicked on the article or not. The algorithm's state variables are updated. One slight modification to the algortihm described in the lecture was that we scaled the reward to compensate the miss rate in the recommendation.
As seen in the lectures, alpha was set to 1+sqrt(ln(2/delta)/2), and delta was set to 0.05 after trying different values.
